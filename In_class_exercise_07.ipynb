{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_class_exercise_07.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muyikem21/Sunday_INFO5731_Spring2021/blob/main/In_class_exercise_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFufRFGNJNi8"
      },
      "source": [
        "# **The seventh in-class-exercise (20 points in total, 3/16/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NmHhlHkJNjG"
      },
      "source": [
        "Question description: In the last in-class-exercise (exercise-06), you collected the titles of 100 articles about data science, natural language processing, and machine learning. The 100 article titles will be used as the text corpus of this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0677gIJNjH"
      },
      "source": [
        "## (1) (8 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLj7ARvvJNjH"
      },
      "source": [
        "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
        "                                          random_state=0, verbose=0)\n",
        "lda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)\n",
        "lda_keys = get_keys(lda_topic_matrix)\n",
        "lda_categories, lda_counts = keys_to_counts(lda_keys)\n",
        "top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
        "for i in range(len(top_n_words_lda)):\n",
        "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crWigK-NJNjI"
      },
      "source": [
        "## (2) (8 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ertkzoAxJNjI"
      },
      "source": [
        "n_topics = 10\n",
        "lsa_model = TruncatedSVD(n_components=n_topics)\n",
        "lsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)\n",
        "def get_keys(topic_matrix):\n",
        "    keys = topic_matrix.argmax(axis=1).tolist()\n",
        "    return keys\n",
        "def keys_to_counts(keys):\n",
        "    count_pairs = Counter(keys).items()\n",
        "    categories = [pair[0] for pair in count_pairs]\n",
        "    counts = [pair[1] for pair in count_pairs]\n",
        "    return (categories, counts)\n",
        "lsa_keys = get_keys(lsa_topic_matrix)\n",
        "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
        "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
        "    top_word_indices = []\n",
        "    for topic in range(n_topics):\n",
        "        temp_vector_sum = 0\n",
        "        for i in range(len(keys)):\n",
        "            if keys[i] == topic:\n",
        "                temp_vector_sum += document_term_matrix[i]\n",
        "        temp_vector_sum = temp_vector_sum.toarray()\n",
        "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
        "        top_word_indices.append(top_n_word_indices)   \n",
        "    top_words = []\n",
        "    for topic in top_word_indices:\n",
        "        topic_words = []\n",
        "        for index in topic:\n",
        "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
        "            temp_word_vector[:,index] = 1\n",
        "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
        "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
        "        top_words.append(\" \".join(topic_words))         \n",
        "    return top_words\n",
        "top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
        "for i in range(len(top_n_words_lsa)):\n",
        "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vct6tsOeJNjJ"
      },
      "source": [
        "## (3) (4 points) Compare the results generated by the two topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYwWryVqJNjJ"
      },
      "source": [
        "LSA generated result is better because it is an information retreival method which works\n",
        "by decomposing the original matrix of words to maintain key topics. On the other hand,\n",
        "LDA assumes documents are produced from a mixture of topics. \n",
        "Those topics then generate words based on their probability distribution. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPtQS6KQQL5R"
      },
      "source": [
        ""
      ]
    }
  ]
}