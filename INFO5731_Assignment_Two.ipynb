{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muyikem21/Sunday_INFO5731_Spring2021/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#CovidVaccine\"](https://twitter.com/hashtag/CovidVaccine) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "# import all the necessary moudles\n",
        "%pip install BeautifulSoup4\n",
        "%pip install requests\n",
        "%pip install pandas as pd\n",
        "%pip json\n",
        "\n",
        "#define the url to extract reviews\n",
        "url='https://www.imdb.com/title/tt7286456/reviews?sort=userRating&dir=desc&ratingFilter=0'\n",
        "\n",
        "#open the url in a new chrome window\n",
        "driver = webdriver.Chrome(executable_path='C:\\webdrivers\\chromedriver.exe')\n",
        " \n",
        "#load all the pages\n",
        "while True:\n",
        "   try:\n",
        "     driver.find_element_by_css_selector('#load-more-cc-objects').click()\n",
        "        \n",
        "#get a response and search for reviews\n",
        "     soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
        "     reviews = soup.findAll('div', attrs={'class': 'text show-more__control'})\n",
        "\n",
        "#get individual review in a list\n",
        "     listOfReviews = [review.get_text().replace('\\n', \"\") for review in reviews]\n",
        "\n",
        "#make a dataframe from list\n",
        "df = pd.DataFrame({'User Reviews': listOfReviews})\n",
        "\n",
        "#save the dataframe on a csv file\n",
        "df.to_csv('userReviews.csv')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Importing Libraries\n",
        "import nltk\n",
        "nltk.download()\n",
        "%pip install pandas as pd\n",
        "%pip install numpy as np\n",
        "%pip install nltk.corpus\n",
        "%pip install stopwords\n",
        "%pip install PorterStemmer\n",
        "%pip install word_tokenize\n",
        "%pip install WordNetLemmatizer\n",
        "%pip install json\n",
        "# Get all stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "# Read the dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "# lemmatization function\n",
        "def lemmatization(string):\n",
        "  txt = [WordNetLemmatizer().lemmatize(x) for x in string]\n",
        "return txt\n",
        "#stemming function\n",
        "def stemming(string):\n",
        "  txt = [PorterStemmer().stem(x) for x in string]\n",
        "return txt\n",
        "# main function\n",
        "def clean_text(dataframe, tt):\n",
        "# Remove the punctuations and noise\n",
        "  dataframe[tt] = dataframe[tt].apply(lambda string: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", string))\n",
        "# Remove the numbers\n",
        "dataframe[tt] = dataframe[tt].apply(lambda string: re.sub(r\"\\d+\", \"\", string))\n",
        "# Remove the stopwords\n",
        "dataframe[tt] = dataframe[tt].apply(lambda string: ' '.join([w for w in string.split() if w not in (stpwordlist)]))\n",
        "# tokenize\n",
        "dataframe['toks'] = dataframe['toks'].apply(lambda x: word_tokenize(x))\n",
        "# Convert to lower string\n",
        "dataframe[tt] = dataframe[tt].str.lower()\n",
        "# Stemming and Lemmatization\n",
        "dataframe['toks'].apply(lambda string: word_stemmer(string))\n",
        "dataframe['toks'].apply(lambda string: word_lemmatizer(string))\n",
        "  \n",
        "return dataframe\n",
        "\n",
        "clean_text(df,\"textColumn\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3GEjMKf8HAl"
      },
      "source": [
        "'''\n",
        "Parse tree: A parse tree is a tree that highlights the syntactical structure of a sequence. Constitency and dependency parsing are two methods that use different types of grammars\n",
        "Constituency Parsing: The constituency parse tree is based on the formalism of context-free grammars (CGF). In this type of tree, the sentence is divided into constituents, that is sub-phases that belong to a specific category in grammar. In english for instance, the phrases, \"a dog\",\"a computer on the table\" are all noun phrases while \"eat a pizza\" and \"go to the beach\" are verb phrases. \n",
        "We use constituency parser when we want to extract sub-phrases from sentence. A constituency parse always contains the words of the sentence as its terminal nodes are represented the constituents of the sentence.\n",
        "\n",
        "Dependency parsing does not make use of phrasal constituents or sub-phrases. The syntax of the sentence is expressed in terms of dependencies between words, that is directed, typed edges between words in a graph.\n",
        "A dependency parse tree is a graph G=(V,E) where the set of vertices, V contains the words in the sentence, and each edge in E connects two words.\n",
        "The graph must satisfy three conditions:\n",
        "1. There has to be a single root node with no incoming edges.\n",
        "2. For each word v in V, there must be a path from root R to V.\n",
        "3.Each note except the root must have exactly \"I\" incoming edge.\n",
        "For example, the word \"saw\" has an outgoing edge of type nsubj to the word \"I\", meaning that \"I\" is the nominal subject of the verb \"saw\". In this case, we say that \"I\" depends on \"saw\". We use dependency parser for several\n",
        "downstream tasks like informatoon extraction or question answering.\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}