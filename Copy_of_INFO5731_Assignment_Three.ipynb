{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muyikem21/Sunday_INFO5731_Spring2021/blob/main/Copy_of_INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcrQCZYh553"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from math import log\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "from numpy import linalg as la\n",
        "\n",
        "datafile = pd.read_csv(r\"C:\\data.csv\")\n",
        "datafile = datafile[['Reviews', 'RevisedReviews']]\n",
        "datafile['RevisedReviews'] = datafile['RevisedReviews'].apply(lambda x: nltk.word_tokenize(x))\n",
        "datafile['N=3 gram'] = datafile['RevisedReviews'].apply(lambda row: list(nltk.ngrams(row, 3)))\n",
        "datafile['number of N=3 gram'] = datafile['N=3 gram'].str.len()\n",
        "\n",
        "\n",
        "def prob_func(x):\n",
        "    probs = []\n",
        "    twograms = list(nltk.ngrams(x, 2))\n",
        "    for (w1, w2) in twograms:\n",
        "        nums = twograms.count((w1, w2))\n",
        "        for w1 in (w1, w2):\n",
        "            denoms = x.count(w1)\n",
        "            probs.append(((w1, w2), nums / denoms))\n",
        "    return probs\n",
        "\n",
        "datafile['Two Gram Chance'] = datafile['RevisedReviews'].apply(lambda x: prob_func(x))\n",
        "\n",
        "datafile.head(5)\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def phrases(vec1):\n",
        "    vec1 = vec1.replace('', '')\n",
        "    vec1 = vec1.lower()\n",
        "    cont1 = []\n",
        "    cont2 = nlp(vec1)\n",
        "    parts = list(cont2.noun_parts)\n",
        "    for part in parts:\n",
        "        cont1.append(part)\n",
        "    return cont1\n",
        "\n",
        "dataframe = pd.DataFrame()\n",
        "dataframe['Reviews'] = data['Reviews']\n",
        "dataframe['Phrases'] = dataframe['Reviews'].apply(lambda x: phrases(x))\n",
        "dataframe['Phrases'] = dataframe['Phrases'].apply(lambda x: [str(item) for item in x])\n",
        "list = dataframe[\"Phrases\"].tolist()\n",
        "list2 = [str(item) for sublist in list for item in sublist]\n",
        "\n",
        "freqlist = Counter(list2)\n",
        "print('Phrase Frequency :', freqlist.most_common(1)[0])\n",
        "FreqCount = freqlist.most_common(1)[0][1]\n",
        "\n",
        "def prob_func2(x):\n",
        "    prob_phrase = []\n",
        "    phrase_cont = []\n",
        "    for Phrases in x:\n",
        "        if Phrases not in phrase_cont:\n",
        "            freq = x.count(Phrases)\n",
        "            rel_prob = round(freq / FreqCount, 5)\n",
        "            prob_phrase.append((Phrases, rel_prob))\n",
        "            phrase_cont.append(Phrases)\n",
        "    return prob_phrase\n",
        "\n",
        "dataframe['probabilities of phrases'] = dataframe['phrases'].apply(lambda x: prob_func2(x))\n",
        "\n",
        "dataframe.head(5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "rev = pd.read_csv(r\"C:\\data\")\n",
        "data2 = rev.set_index('ReviewNumber').T.to_dict('Column')\n",
        "\n",
        "def Conversion(s):\n",
        "    containerstring = \" \"\n",
        "    return (containerstring.join(s))\n",
        "\n",
        "data2 = {doc_label: Conversion(s) for doc_label, s in data2.items()}\n",
        "\n",
        "container4 = {doc_label: nltk.word_tokenize(text) for doc_label, text in data2.items()}\n",
        "\n",
        "numberofoccurances = {doc_label: Counter(tok) for doc_label, tok in container4.items()}\n",
        "\n",
        "container5 = set()\n",
        "for counter in numberofoccurances.values():\n",
        "    container5 |= set(counter.keys())\n",
        "container5 = sorted(list(container5))\n",
        "\n",
        "container6 = []\n",
        "for counter in numberofoccurances.values():\n",
        "    container6rows = [counter.get(term, 0) for term in container5]\n",
        "    container6.append(container6rows)\n",
        "labels = list(numberofoccurances.keys())\n",
        "\n",
        "container7 = np.mat(container6, dtype=float)\n",
        "container8 = container7 / np.sum(container7, axis=1)\n",
        "\n",
        "def function3(t, docs):\n",
        "    return sum(t in d for d in docs.values())\n",
        "\n",
        "def function4(t, docs):\n",
        "    a = function3(t, docs)\n",
        "    return log(1 + len(docs) / (1+a))\n",
        "\n",
        "container9 = [function4(t, container4) for t in container5]\n",
        "container10 = np.mat(np.diag(container9))\n",
        "container11 = container8 * container10\n",
        "\n",
        "container12 = np.array(container11)\n",
        "container13 = pd.DataFrame(data=container12, index=rev['review_id'], columns=container5)\n",
        "container13.head()\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def function6(query):\n",
        "    tempcontainer = tempcontainer.replace('[^\\w\\s]', '')\n",
        "    tempcontainer = tempcontainer.replace('\\d+', '')\n",
        "    tempcontainer = \" \".join(x for x in tempcontainer.split() if x not in stop)\n",
        "    tempcontainer = \" \".join(x.lower() for x in tempcontainer.split())\n",
        "    tempcontainer = \" \".join([PorterStemmer().stem(word) for word in tempcontainer.split()])\n",
        "    tempcontainer = \" \".join([Word(word).lemmatize() for word in tempcontainer.split()])\n",
        "    return tempcontainer\n",
        "\n",
        "def function14(query):\n",
        "    tempcontainer2 = {'query': function6(query)}\n",
        "\n",
        "    tempcontainer3 = {doc_label: nltk.word_tokenize(text) for doc_label, text in tempcontainer2.items()}\n",
        "    tempcontainer4 = {doc_label: Counter(tok) for doc_label, tok in tempcontainer3.items()}\n",
        "\n",
        "    container20 = []\n",
        "    for counter in tempcontainer4.values():\n",
        "        container21 = [counter.get(term, 0) for term in container5]\n",
        "        container20.append(container21)\n",
        "    container22 = list(tempcontainer4.keys())\n",
        "\n",
        "    container23 = np.mat(container20, dtype=float)\n",
        "    container24 = container23 / np.sum(container23, axis=1)\n",
        "\n",
        "    container25 = [function4(t, tempcontainer3) for t in container5]\n",
        "    container26 = np.mat(np.diag(container25))\n",
        "    container27 = container24 * container26\n",
        "\n",
        "    container28 = np.array(container27)\n",
        "    return container28\n",
        "\n",
        "def cossimfunc(input1, input2):\n",
        "    input2 = np.array(input2)\n",
        "    input1 = function14(input1)\n",
        "    if la.norm(input1) != 0 and la.norm(input2) != 0:\n",
        "        cossimis = np.inner(input1, input2) / la.norm(input1) * la.norm(input2)\n",
        "    else:\n",
        "        cossimis = 0\n",
        "    return float(cossimis)\n",
        "\n",
        "\n",
        "stringcontainer = \"the movie is extraordinary\"\n",
        "\n",
        "cossimcontainer = []\n",
        "for i in range(len(container13)):\n",
        "    cossimcontainer2 = cossimfunc(stringcontainer, container13.iloc[i])\n",
        "    cossimcontainer.append(cossimcontainer2)\n",
        "\n",
        "cossimresult = pd.DataFrame({'The Review': labels, 'Cosine Similarity Result': cossimcontainer})\n",
        "cossimresult.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "\n",
        "\n",
        "Sunday_INFO5731_Spring2021/THEASSIGNMENT.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}