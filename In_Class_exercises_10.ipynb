{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_Class_exercises-10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muyikem21/Sunday_INFO5731_Spring2021/blob/main/In_Class_exercises_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kosYhVr7E9NI"
      },
      "source": [
        "# In class exercise 10 (20 points in total, 4/16/2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne7UtSIIE9Nb"
      },
      "source": [
        "The purpose of the exercise is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K means, \n",
        "DBSCAN,\n",
        "Hierarchical clustering. \n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDGg5OZDE9Nk"
      },
      "source": [
        "#importing the libraries\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import seaborn as sns\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "\n",
        "#reading in the data\n",
        "sample=pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
        "\n",
        "#defining cut off partition\n",
        "def partition(vec):\n",
        "    if vec < 3:\n",
        "        return 'negative'\n",
        "    return 'positive'\n",
        "\n",
        "ratingmodified = sample['Rating']\n",
        "countsratings = ratingmodified.map(partition) \n",
        "sample['Rating'] = countsratings\n",
        "\n",
        "#removing the duplicates\n",
        "sorteddata=sample.sort_values('Product Name', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
        "withoutduplicates=sorteddata.drop_duplicates(subset={\"Brand Name\",\"Price\",\"Rating\",\"Reviews\"}, keep='first', inplace=False)\n",
        "sns.countplot(final.Rating)\n",
        "\n",
        "#K means model\n",
        "wordcount = CountVectorizer()\n",
        "wordcounts = wordcount.fit_transform(final['Rating'].values)\n",
        "termscount = wordcount.get_feature_names()\n",
        "\n",
        "kmeansmodel = KMeans(n_clusters = 10,init='k-means++', n_jobs = -1,random_state=99)\n",
        "kmeansmodel.fit(wordcounts)\n",
        "\n",
        "# DBSCAN model\n",
        "ratinslist=[]\n",
        "for ratinscleaned in final['Reviews'].values:\n",
        "    ratinscleanded=[]\n",
        "    ratinscleaned=cleanhtml(ratinscleaned)\n",
        "    for w in ratinscleaned.split():\n",
        "        for Reviews in cleanpunc(w).split():\n",
        "            if(Reviews.isalpha()):    \n",
        "                ratinscleanded.append(Reviews.lower())\n",
        "            else:\n",
        "                continue \n",
        "    ratinslist.append(ratinscleanded)\n",
        "\n",
        "modelwordtovector=gensim.models.Word2Vec(ratinslist,size=100, workers=4)\n",
        "\n",
        "ratinscleaned_vectors = []; \n",
        "for ratinscleaned in ratinslist: \n",
        "    ratinscleaned_vec = np.zeros(100) \n",
        "    cnt_words =0; \n",
        "    for word in ratinscleaned:\n",
        "        try:\n",
        "            vec = modelwordtovector.wv[word]\n",
        "            ratinscleaned_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    ratinscleaned_vec /= cnt_words\n",
        "    ratinscleaned_vectors.append(ratinscleaned_vec)\n",
        "ratinscleaned_vectors = np.array(ratinscleaned_vectors)\n",
        "ratinscleaned_vectors = np.nan_to_num(ratinscleaned_vectors)\n",
        "ratinscleaned_vectors.shape\n",
        "\n",
        "DBSCANmodel = DBSCAN(eps = 4, min_samples = minPts, n_jobs=-1)\n",
        "DBSCANmodel.fit(ratinslist)\n",
        "Rating['Amazon_Unlocked_Mobile'] = model.labels_\n",
        "\n",
        "#clustering model\n",
        "dendroword=hierarchy.dendrowordgram(hierarchy.linkage(sent_vectors,method='ward'))\n",
        "plt.axhline(y=10)\n",
        "\n",
        "clusters = cluster2lomerativeclustersing(n_clusterss=4, affinity='euclidean', linkage='ward') \n",
        "cluster2=clusters.fit_predict(sent_vectors)\n",
        "Rating['Amazon_Unlocked_Mobile'] = clusters.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3r0uX15E9Nl"
      },
      "source": [
        "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jdiFI2uE9Nl"
      },
      "source": [
        "Comparison of K means, DBSCAN and Hierarchical clustering are as follow.\n",
        "Hierarchical clustering is the slowest of the two algorithms while K means \n",
        "is the least accurate. Consequently, the best of the 3 algorithms is the DBSCAN algorithm.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}